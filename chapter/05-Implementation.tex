\chapter{Implementing and Optimising}
\label{chap:implementation}

Now that we have a foundation for regularity aware flattening of nested data parallel programs we will explore what is required to use our program transform in an already implemented, expressive, powerful, high-performance array language and compiler. We will provide a high-level overview of the the implementation. For more detailed information we refer to the publicly available implementation.\footnote{This is available at \url{https://github.com/AccelerateHS/accelerate/tree/feature/sequences}}

In this chapter, we will first revisit Accelerate's compilation pipeline (from Chapter~\ref{chap:background}) in order to see how our sequences extension fits into that, before exploring the implementation of the flattening transform itself. Following this, we will describe how we extended Accelerate's optimisation passes in order to achieve competitive absolute performance of our sequence programs. Finally, we will talk about the dynamic component of the scheduling necessary to execute sequence computations.

\section{Architecture}
As discussed in Chapter~\ref{chap:background}, Accelerate has a fairly standard compiler architecture. It is a pipeline that takes a higher-order AST and ultimately produces LLVM IR which it passes to a the LLVM compiler in order to generate native code.

Prior to the sequences extension Accelerate's pipeline could be arranged into these stages.

\begin{itemize}
%
\item Sharing recovery -- Converts the higher order abstract syntax (HOAS) into first order syntax with Debruijn indices and also recovers sharing.
%
% \item Segment rewriting -- If necessary, will rewrite explicit segment operations such that segment descriptors are converted into the right format. This will be covered in more depth below. For now, it is just another compilation stage.
%
\item Fusion and optimisation -- Try to minimise unnecessary array traversals and optimise for optimal code generation. This produces a an AST with explicit delayed arrays.
%
\item Code generation -- Generate LLVM IR from AST and pass it off to the LLVM compiler for compilation into binaries for the target architecture. This is either x86 (with vector instructions) or PTX, for CUDA capable GPUs.
%
\item Execution -- Schedule and execute the computations on the target hardware.
\end{itemize}

In order to support sequence computations, we both have to extend the AST with a set of sequence combinators and change the pipeline to support them. The changes needed, at the high-level, are as follows:
%
\begin{enumerate}
%
\item Update sharing recovery to support the additional sequence AST constructors. This will not be explored in detail as it is not novel or different to how Accelerate handles other AST forms. See \citet{McDonell:acc-optim} for details on how sharing recovery works.
%
\item Add an additional flattening stage after sharing recovery to flatten the computations embedded in sequence computations. The theory of this was presented in Chapter~\ref{chap:theory} and its implementation is described below.
%
\item Extend the fusion system to exploit additional fusion opportunities presented by sequence computations.
%
\item An addition to the runtime system to schedule sequence computations in a way where memory usage can be minimised while maximising utilisation.
%
\end{enumerate}

In the next section, we will address (2). (3) will be tackled in the the following section. (4) will follows after that. Firstly however, we have to be more concrete about how we represent sequences.

% \TODO{The sequences AST. This is gonna suck.}

As introduced in Section~\ref{sec:sequences}, a sequence computation @Seq [Array sh b]@ adds a second level of nested, irregular data-parallelism on top of the flat regular parallelism of an array computation @Acc (Array sh b)@. Hence, we can generally regard such a sequence computation as a mapping of a flat, regular function @f@ over an irregular sequence @xs@:
%
\begin{lstlisting}
Seq [Array sh b]  %$\sim$%  mapSeq f xs
  where
    f  :: Acc (Array sh' a) -> Acc (Array sh b)
    xs :: Seq [Array sh' a]
\end{lstlisting}
%
In addition to the \emph{inner-function} parallelism in @f@, we want to
exploit as much of the parallelism of the outer @mapSeq@ (the \emph{intra-function} parallelism) as possible on a
given architecture to achieve optimal performance. That is, we execute multiple ---but not necessarily \emph{all}--- elements of the @mapSeq@ in
parallel. Avoiding the need to utilise all outer parallelism helps us to support out-of-core datasets on GPUs, by only loading into
device memory those elements of the stream @xs@ that are processed together. By
dynamically adjusting the number of elements operated on at once, we aim to
minimise memory usage while aiming to keep all processing elements fully utilised. The flattening transformation takes the definition of the function @f@ and rewrites it to a definition for @f@$^\uparrow$, such that semantically @f@$^\uparrow$@ = mapSeq f@; in other words, @f@$^\uparrow$ can process multiple elements of @xs@ at once, in parallel.

% The more general case, where a sequence computation computes a tuple of sequences is more complex.
% %
% \begin{lstlisting}
% Seq ([Array sh1 b1], [Array sh2 b2])  %$\sim$%  (mapSeq f xs, mapSeq f ys)
%   where
%     f  :: Acc (Array sh' a) -> Acc (Array sh b)
%     xs :: Seq [Array sh' a]
% \end{lstlisting}
% %

\section{Flattening}
The design of the flattening compiler stage closely follows the what is described in~\ref{sec:flattening}. However, it differs in a few key areas.
%
\begin{itemize}
\item There are considerably more combinators in Accelerate than \ndp{}. This means there needs to be lifted versions of nearly all combinators. In a lot of cases there is both a regular and irregular lifted equivalent.
%
\item Accelerate is stratified into two separate sublanguages. Scalar computations are represented by the @Exp@ language and array computations by @Acc@. @Exp@ terms are embedded in @Acc@ terms.
%
\item There are no nested arrays and sequences are only one level of nesting. This means that the implementation of the transform can be simplified in a few places, but fundamentally remains the same.
%
% \item Accelerate supports conditional execution through an @if-then-else@ like combinator.
%
\item Accelerate uses representation types to handle tuples of multiple arity. In \ndp we just assumed that we could easily generalize over tuples of arbitrary arity.
%
\end{itemize}
%
We will address these points as we give an overview of the flattening implementation. Like we did with \ndp{}, we will begin by first describing the type level component before moving on to the actual program transform.

\subsection{Types}
Recall that in order to represent values in our language at a higher dimension, we had to first normalise then flatten types. This was captured by these relations.
%
\begin{lstlisting}[style=ndp]
data Norm t t' where
   Scalar :: IsScalar e
          => Norm e (Array Z e)
   Nest   :: Norm e (Array sh_1 e_1, Array sh_2 e_2, ..., Array sh_n e_n)
          -> Norm (Array sh e) (Array (sh ++ sh_1) e_1, Array (sh ++ sh_2) e_2, ..., Array (sh ++ sh_n) e_n)
   Tuple%$_\mathcal{N}\,$%:: (Norm t_1 t_1', Norm t_2 t_2', ..., Norm t_n t_n')
          -> Norm (t_1, t_2, ..., t_n) (t_1', t_2', ..., t_n')

data Vect ctx t t_flat where
   Avoid_S    :: IsScalar t
             => Vect ctx t t                                             -- avoid vectorisation
   Avoid_A    :: IsScalar e
             => Vect ctx (Array sh e) (Array sh e)
   Regular   :: Norm t (Array sh_1 e_1, Array sh_2 e_2, ..., Array sh_n e_n)   -- regular context
             -> Vect ctx t ( Array (ctx++sh_1) e_1
                         , Array (ctx++sh_2) e_2
                         , ...
                         , Array (ctx++sh_n) e_n)
   Irregular :: Norm t (Array sh_1 e_1, Array sh_2 e_2, ..., Array sh_n e_n)   -- irregular context
             -> Vect ctx t ( (Segments (ctx++sh_1), Vector e_1)
                         , (Segments (ctx++sh_2), Vector e_2)
                         , ...
                         , (Segments (ctx++sh_n), Vector e_n))
   Tuple_F    :: (Vect ctx t_1 t_1_flat, Vect ctx t_2 t_2_flat, ..., Vect ctx t_n t_n_flat)
             -> Vect ctx (t_1, t_2, .., t_n) (t_1_flat, t_2_flat, ..., t_n_flat)
\end{lstlisting}
%
Because of the differences between Accelerate and \ndp above, in the implementation we combine both of these relations into a single GADT.
%
\begin{lstlisting}
data LiftedType t t' where
   UnitT       ::                      LiftedType ()           ()
   LiftedUnitT ::                      LiftedType ()           (Scalar Int)
   AvoidedT    :: (Shape sh, Elt e) => LiftedType (Array sh e) (Array sh e)
   RegularT    :: (Shape sh, Elt e) => LiftedType (Array sh e) (Array (sh:.Int) e)
   IrregularT  :: (Shape sh, Elt e) => LiftedType (Array sh e) (Segments sh, Vector e)
   TupleT      :: (IsProduct Arrays t, IsProduct Arrays t')
               => LiftedTupleType (TupleRepr t) (TupleRepr t')
               -> LiftedType t t'
\end{lstlisting}
%
It is important to note that this structure is not parameterised by the lifting context. The reason for this is the lack of deep nesting in Accelerate. As our only nested structure is the sequence, and sequences themselves cannot be nested, the only possible nested context is being an element of a sequence. Hence, we can represent all possible contexts with an @Int@.

The first two constructors deal with the unit type. This is something we didn't encounter in Chapter~\ref{chap:theory}. In short, the unit type (@()@) in a lifted context can just be represented by the context itself. There is only one possible unit type, so a collection of elements of that type can be represented by just the shape of the collection. For a sequence, that context is just an int indicating the length of the sequence (or the size of the current chunk of the sequence; more on that below).

The @AvoidedT@ constructor is equivalent to \lstinline[style=ndp]{Avoid_A}. In the array language of Accelerate (@Acc@), scalar values do not occur so \lstinline[style=ndp]{Avoid_S} is not needed.

@RegularT@ and @IrregularT@ are equivalent to @Regular@ and @Irregular@ respectively, but without being generalised over tuples. For the regular case, we simply add the lifting context to the dimensionality of the array. This is equivalent to \lstinline[style=ndp]{ctx++sh} but @ctx@ in this case is just @Z:.Int@. For the irregular case, we have a slightly different notion of segment descriptors than before. The context is implicit in the construction. What this means is that @Segments (sh:.Int)@ in \ndp{} is equivalent to @Segments sh@ in Accelerate. The outer dimension is assumed to always be present.

In order to correctly handle tuples, we have the @TupleT@ constructor. It captures the @IsProduct@ constraint on both the original type @t@ and the transformed type @t'@. In essence, it means they they are both products of which all components are members of the @Arrays@ type class. In order to specify the lifted type of each component in this product we use @LiftedTupleType@.
%
\begin{lstlisting}
data LiftedTupleType t t' where
   NilLtup  :: LiftedTupleType () ()
   SnocLtup :: (Arrays a, Arrays a')
            => LiftedTupleType t t'
            -> LiftedType a a'
            -> LiftedTupleType (t,a) (t',a')
\end{lstlisting}
%
Recall from Chapter~\ref{chap:background} that the way Accelerate uses representation types is that tuples are represented as left nested tuples.


\subsection{Segment representation}

In Chapter~\ref{chap:theory} we left the issue of how to represent segment descriptors as abstract and implementation specific. Now that we are discussing the implementation, we need to address that. We will start by giving the most general definition of segment descriptors and then show how by specialising it to our sequences concept, we can arrive at a more optimal definition. To start with, let's suppose we want to represent segment descriptors for an irregular 3D structure. To do that, a simple definition is this.
%
\begin{lstlisting}[style=ndp]
Segments_NDP (Z:.Int:.Int:.Int) = (Vector Int, Vector Int)
\end{lstlisting}
%
Given a 3D index, we can determine the location of the corresponding value in the value vector. This looks like this:
%
\begin{lstlisting}[style=ndp]
(#) :: Segmments_NDP (Z:.Int:.Int:.Int) -> Z:.Int:.Int:.Int -> Int
(k,j) @ (Z:.z:.y:.x) = let kz = k !! z
                           jy = j !! (kz + y)
                           ix = jy + x
                       in ix
\end{lstlisting}
%
Here, we're taking the index into the 3D structure @Z:.z:.y:.x@ and converting it into its corresponding position in the values vector.

Suppose our structure has an outer dimension of size $l$, the average size of the middle dimension is $m$, and the average size of the inner dimension is $n$. This representation for segment descriptors is a vector of length $l$ and a vector of max length $l \times m$.

However, the best representation depends on precisely where the irregularity is. For example, the following representation,
%
\begin{lstlisting}[style=ndp]
Segments_NDP (Z:.Int:.Int:.Int) = Vector (Z:.Int:.Int, Int)
\end{lstlisting}
%
works as long as the 3D structure is a vector of 2D arrays. The final offset can be calculated like so
%
\begin{lstlisting}[style=ndp]
(#) :: Segmments_NDP (Z:.Int:.Int:.Int) -> Z:.Int:.Int:.Int -> Int
segs # (Z:.z:.y:.x) = let (sh,off) = segs !! z
                      in off + toIndex sh (Z:.y:.x)
\end{lstlisting}
%
This is the multidimensional equivalent of the size-offset representation in Chapter~\ref{chap:background}. This representation is size $l$. Therefore it's space requirement is significantly less than the more general representation. Furthermore, implementing @(#)@ only requires a single indexing operation.

Returning to our concept of sequences, we see that we only need this second way of representing the segment descriptors. Sequences can only contain regular arrays, so we only have irregularity in the outer dimension. Therefore our definition for @Segments@ is
%
\begin{lstlisting}[style=ndp]
Segments_NDP (sh:.Int) = Vector (sh, Int)
\end{lstlisting}
%
For Accelerate, we make some small changes. Firstly, as we only ever have segment descriptors of dimension at least one, we can ignore that part of the parameter. Secondly, it is better for Accelerate's optimisation pipeline to store the offsets and the shapes of subarrays in two separate vectors (this will be explained in more detail below). Lastly, we often need access to the total size of all segments. This can be calculated by taking the last offset and the size of the last shape and adding them together, but doing this calculation each time we need this value interferes with other optimisations. Hence, we store this value separately.
%
\begin{lstlisting}[style=ndp]
Segments sh = (Vector sh, Vector Int, Scalar Int)
\end{lstlisting}
%



% Fully captures all information about the layout of structure, but only in the case when the irregularity is introduced in the outer dimension. This would be fine for a vector of 2D arrays; i.e. the outer dimension determines both the middle and inner dimensions. If our 3D structure is a 2D array of vectors though, then this would be a more appropriate representation.
% %
% \begin{lstlisting}[style=ndp]
% Segments (Z:.Int:.Int:.Int) = Array (Z:.Int:.Int) (Z:.Int, Int)
% \end{lstlisting}
% %
% In this case, the outer and middle dimension determine the inner one. If we wish to handle both cases, then we need to split it up further.
% %
% \begin{lstlisting}[style=ndp]
% Segments (Z:.Int:.Int:.Int) = (Array (Z:.Int) (Z:.Int), Array (Z:.Int) (Z:.Int))
% \end{lstlisting}
% %
% Here, we determine the middle dimension from the outer one and the inner one from the middle one. This is equivalent to a vector of vectors of vectors, the most general 3D structure.

%
% \begin{lstlisting}[style=ndp]
% Segments (sh_1 ++ sh_2 ++ sh_3 ++ ... ++ sh_%$_{n-1}$% ++ sh_n)
%   = (Array sh_1 sh_2, Array sh_2 sh_3, ..., Array sh_%$_{n-1}$% sh_n)
% \end{lstlisting}
%

%
% \begin{lstlisting}[style=ndp]
% segs :: Segments_1 (Z:.Int:.Int)
% segs = {Z:.4:.3, Z:.4:.2, Z:.4:.4, Z:.4:.1}
% \end{lstlisting}
% %
% Would be the segment descriptor for a vector of length 4 containing vectors of length 3,2,4 and 1
% This is a simple representation that seems to capture all the information we require, but is a somewhat redundant representation, in that the outer dimension of all shapes in the vector is always going to be 4. If we look at the operations we need to perform on segments then @segmented@ is trivial:
% %
% \begin{lstlisting}[style=ndp]
% segmented :: Array sh sh' -> Segments_1 (sh++sh')
% segmented arr = flatten $ map (indexInit (shape arr) ++) arr
% \end{lstlisting}
% %
% The @#@ operator presents problems though.
% %
% \begin{lstlisting}
% (#) :: Segments_1 sh -> sh -> Int
% arr # ix = let offsets = scanl (+) 0 (map shapeSize arr)
%            in  offsets ! indexLast ix
%              + toLinearIndex (arr ! indexLast ix) (indexInit ix)
% \end{lstlisting}
% %
% Here, we're having to do two things in order to determine the index into the values vector for a given index. Firstly, we have to calculate the offset of each subarray. Then, secondly, we take this offset and add it to the linear index of @ix@ in the extent of the subarray. We do the first of these steps with a left-scan (prefix-sum), but this is a costly operation. In addition, it is not scalar so we can't use this version of (@#@) in a nested context. For this reason, we want to do better.


\subsection{Transformation}
With a concrete representation for segment descriptors, we are able to fully implement the lifting transform in Accelerate.

In terms of key differences between the flattening of \ndp{} and Accelerate, we will first start by looking at how we represent the environment. Recall that \ndp{} had an environment of the form.
%
\begin{lstlisting}[style=ndp]
data Env ctx Gamma Gamma_flat where
  []    :: Env Z [] []
  (:)   :: Vect t t_flat                      -- standard free variable
        -> Env ctx Gamma Gamma_flat
        -> Env ctx (t : Gamma) (t_flat : Gamma_flat)
  (:_R)  :: Shape sh                      -- regular flattening context
        => Expr Gamma_flat sh
        -> Env ctx Gamma Gamma_flat
        -> Env (ctx++sh) Gamma Gamma_flat
  (:_Ir) :: Shape sh                      -- irregular flattening context
        -> Expr Gamma_flat (Segments sh)
        -> Env ctx Gamma Gamma_flat
        -> Env (ctx++sh) Gamma Gamma_flat
\end{lstlisting}
%
In Accelerate, we have the same structure. The main differences are that Accelerate has to two separate environments, one for scalar values and one for array values, and that, once again, we aren't making the lifting context explicit in the type.
%
\begin{lstlisting}
data Context acc aenv aenv' where
  BaseC     :: Context acc aenv aenv

  PushC     :: Arrays t'
            => Context acc aenv aenv'
            -> LiftedType t t'
            -> Nesting acc (aenv', t')
            -> Context acc (aenv, t) (aenv', t')

data Nesting acc aenv = NoNest
                      | RegularNest (acc aenv (Scalar Int))
                      | forall sh. Shape sh => IrregularNest (acc aenv (Segments sh))
\end{lstlisting}
%
We also combine the different cons-like constructors into one with an additional \emph{nesting indicator} that indicates whether this cons cell also captures nesting details. The @acc@ type parameter exists as a consequence of the \emph{tying the recursive knot} trick described in Chapter~\ref{chap:background}.

The transform itself proceeds in much the same way as for \ndp{}. The result of the lifting transform, which in \ndp{} was
%
\begin{lstlisting}[style=ndp]
exists t_flat. (Vect ctx t t_flat, Expr Gamma_flat t_flat)
\end{lstlisting}
%
can be directly converted into this GADT.
%
\begin{lstlisting}
data LiftedAcc acc aenv t where
  LiftedAcc :: Arrays t' => LiftedType t t' -> acc aenv t' -> LiftedAcc acc aenv t
\end{lstlisting}
%

\subsection{Lifted operations}

In \ndp{} we had to have @fold_seg@ and @generate_seg@ as the segmented versions of existing operations. In Accelerate we require both of these as well as segmented versions of a number of other combinators. Some of these are available as language primitives, meaning each backend has its own implementation. Others, however, have to be implemented with the existing language constructs.

As described in Chapter~\ref{chap:background}, the internal syntax of Accelerate is first-order and uses typed Debruijn indices. Consequentially, the lifted version of operations which are specialised and spliced into the lifted version of an array program, must also be written in this way. In this section, to take we take the liberty of instead writing them in the familiar Accelerate frontend language. This for clarity and brevity.

We can divide Accelerate's array combinators into roughly these categories:
%
\begin{itemize}
\item  Producers -- This includes @generate@, @map@, @zipWith@ and @backpermute@. All of these can be implemented in terms of @generate@ with indexing.
\item Folds -- This includes @fold@ and @fold1@.
\item Scans -- This includes @scanl@, @scanr@, @scanl'@, @scanr'@.
\item Stencil -- Accelerate has built-in support for stencil operations. We don't support these with sequence computations, leaving it for future work.
\end{itemize}

What follows is an explanation for how we implement the lifted irregular versions of each of the above categories. For the regular versions, the combinators are already rank-polymorphic so little additional work is necessary. For lifted irregular, or segmented, operations we typically have to do more involved computations involving the segment descriptors.

\subsubsection{Producers}
As all of these combinators can be implemented in terms of @generate@ their segmented versions can be implemented in terms of a segmented @generate@. Therefore that is what needs to be implemented.
%
\begin{lstlisting}
generateSeg :: (Shape sh, Elt e)
            => Acc (Segments sh)
            -> (Exp Int -> Exp sh -> Exp e)
            -> Acc (Vector e)
generateSeg (unlift -> (shapes, offsets, totalSize)) f
  = map (\(unlift -> (n,i)) -> f n (toIndex (shapes ! n) i)) indices
  where
    ones      = fill (index1 totalSize) (-1,1)
    zeroes    = generate (shape offsets) (\ix -> lift (unindex1 ix, 0))
    heads     = scatter offsets ones zeroes
    indices   = scanl1 (.+) heads
    (unlift -> (a,b)) .+ (unlift -> (a',b')) = a == a' ? (lift (a, b+b'), lift (a',0))
\end{lstlisting}
%
This definition may appear surprisingly complex. This observation is correct, and gives a further justification to the point of this work, that writing flattened arrays programs by hand is hard but recognising and taking advantage of regularity is also important.

We first create a vector of pairs of the total size of the segmented array (@ones@). The first and second component of each pair are a $-1$ and a $1$ respectively. We use $-1$ here because this value needs to be outside the range of valid indices. We then create an indexed vector that is the same length as the offsets vector (@zeroes@). It contains the index of the element as the first component of each pair and 0 as the second. Using these two vectors, we can then compute a vector of head flags (@heads@). This vector has a pair of (-1,1) at all positions except for those at the start of each segment where there is the number of the segment and a 0. We do this via @scatter@.
%
\begin{lstlisting}
scatter :: Elt e
        => Acc (Vector Int)           -- destination indices to scatter into
        -> Acc (Vector e)             -- default values
        -> Acc (Vector e)             -- source values
        -> Acc (Vector e)
\end{lstlisting}
%
This allows us to position the head flags at the correct offsets into the large vector of offsets.

By performing a scan over this flags vector, we are able to construct the indices for the segmented generate. @indices@ is a vector of pairs where the first component is the number of the segment that element occurs in and the second component is the index of that element within the segment. Using these indices we can then compute the result of the segmented generate.

\subsubsection{Folds}
Accelerate has two non-segmented versions of fold, @fold@ and @fold1@. Both of these need flattened equivalents. Fortunately, this is simpler than flattening generate as Accelerate provides native @foldSeg@ and @foldSeg1@. We still have to do some work in order to get our input in the right form for these operations, but we do not have to fully implement it or make scheduling decisions in the process of flattening.

Here is the type signature for @foldSeg@ in Accelerate.
%
\begin{lstlisting}
foldSeg :: (Shape sh, Elt a, Elt i, IsIntegral i)
        => (Exp a -> Exp a -> Exp a)
        -> Exp a
        -> Acc (Array (sh:.Int) a)
        -> Acc (Vector i)
        -> Acc (Array (sh:.Int) a)
\end{lstlisting}
%
First observe that it expects a vector of integral segment descriptors, but a potentially multidimensional array of values. This version of a segmented fold actually allows for multiple segmented reductions to be performed over the inner dimension of an array. This is actually more general than we require. For the purposes of explanation, we will specialise it to
%
\begin{lstlisting}
foldSeg1D :: Elt a
          => (Exp a -> Exp a -> Exp a)
          -> Exp a
          -> Acc (Vector a)
          -> Acc (Vector Int)
          -> Acc (Vector a)
foldSeg1D = foldSeg
\end{lstlisting}
%
This gives us something close to the segmented folds described in Chapter~\ref{chap:background}. Using this, we can build the segmented fold we had in Chapter~\ref{chap:theory}. First, we need to convert our higher dimensional segment descriptors into the integral segment descriptors.
%
\begin{lstlisting}
segSizes :: Shape sh => Acc (Segments sh) -> Acc (Vector Int)
segSizes (unlift -> (shapes, _, _)) = map ShapeSize shapes
\end{lstlisting}
%
Then, we apply @foldSeg1D@ to the integral segments and the values vector. This gives us the values vector of the result, but we still need to compute the segment descriptors.

% Recall, from Chapter~\ref{chap:theory}, that the extent of a result of a fold is not trivial to compute. All zeroes in the extent become ones. For example, an array of extent
% %
% \begin{lstlisting}
% Z:.3:.2:.1
% \end{lstlisting}
% %
% produces an array of extent
% %
% \begin{lstlisting}
% Z:.3:.2
% \end{lstlisting}
% %
% after folding. However, if instead the array has extent
% %
% \begin{lstlisting}
% Z:.0:.3:.0:.2
% \end{lstlisting}
% %
% after folding the result will have extent
% %
% \begin{lstlisting}
% Z:.1:.3:.1
% \end{lstlisting}
%
We do that with the following function.
%
\begin{lstlisting}
foldSegments :: Shape sh => Acc (Segments (sh:.Int)) -> Acc (Segments sh)
foldSegments (lift -> (shapes, _, _)) = segsFromShapes (map indexTail shapes)
\end{lstlisting}
%
Here, @segsFromShapes@ computes segment descriptors from a vector of shapes.
%
\begin{lstlisting}
segsFromShapes :: Shape sh => Acc (Vector sh) -> Acc (Segments sh)
segsFromShapes shapes = let (offs, totalSize) = scanl' shapes
                        in (shapes, offs, totalSize)
\end{lstlisting}
%
The @scanl'@ combinator computes the left-scan of its input, but rather than returning a vector of length $n+1$, where $n$ is the size of the input, it returns a vector of length $n$ and a scalar value containing the that final element.

\subsubsection{Scans}

For the segmented version of the various scans, we can define them in terms of segmented generate and non-segmented scans. The approach taken is similar for all the different variants. We described the simplest variant here @scanl1@ and refer to the representation for the implementations of segmented @scanl@, @scanr@, @scanr1@, @scanl'@, @scanr'@, etc.
%
\begin{lstlisting}
scanl1Seg :: (Exp e -> Exp e -> Exp e)
          -> Acc (Segments (sh:.Int))
          -> Acc (Vector e)
          -> Acc (Vector e)
scanl1Seg f z segs vals
  = map fst (scanl1 f' (zip vals flags))
  where
    flags = generate_seg segs (\n (unlift -> sh:.i) -> i == 0)
    f' (e,_) (e',flag) = (flag ? (e', f e e'), false)
\end{lstlisting}
%

We first compute a flags vector that is the same length as the values vector. It contains @true@ at the start of each segment and @false@ everywhere else. We then create a version of the binary function @f@ that only applies it when this flag is @false@. This technique of tagging the values with a flag indicating whether they are at the start of a segment is similar to previous approaches\citep{Blelloch:1990vl}.


% \section {Vectorising data transfer}
% \TODO{Do I want this?}

% An important consideration for vectorisation is how to vectorise @subarrays@. With subarrays, we want to create a sequence by splitting an array up into subarrays. In the case of 1D arrays (vectors) this is trivial. To take continuous subvectors from a vector we can just take a single larger subvector. For 2D arrays (matrices) this is harder. Supposing we were using a chunk size of 3 and wanted to split up the following array, the first chunk could be copied with one call to the @memcpy2D@ method CUDA provides.

% \TODO{Include diagram of a matrix with 5x5 submatrices.}

% While this chunk that has been extracted has its subarrays concatenated on the inner dimension, that can be simply fixed by a backwards permutation once it has been transferred. Thanks to Accelerate's \emph{fusion} system, this permutation will fuse into the computation consuming the chunk and thus not create an intermediate array.

% The 2nd chunk, however, goes over 2 columns. The only way to resolve this is to do 2 memcpys like so:

% \TODO{Include diagram for 2nd chunk}

% Provided the chunk size is less than the height of the array, we can always copy a chunk with no more than 2 memcpys. If the chunk size is more than the height of the array, we have to perform an additional transfer.

% Take this different example. Supposing a chunk size of 7, we would need to transfer the first and second chunks as shown in these diagrams.

% \TODO{Include diagrams for the 1st and 2nd chunk of a matrix with 3x7 submatrices}

% In general, our chunks will always contain three components, which we'll call the head, body, and tail, each of which gets transferred with one @memcpy2d@ and permuted afterward.

\section{Fusion}
\label{sec:Optimisation}

While we have shown that it is possible to take our sequence programs with nesting and transform them so that the nesting is removed, doing so has introduced one particular performance problem that needs to be addressed. In the process of flattening, we introduce \emph{numerous} extra intermediate arrays. The na\"ive compilation of programs that contain so many intermediate arrys quickly drags down performance as much of the programs run time is spent reading and writing values to and from memory. Fusion
(or deforestation) attempts to remove this overhead
by combining adjacent transformations on data structures in order to remove
intermediate results. This has been studied
extensively~\cite{Wadler:1990ix,Coutts:stream-fusion,Lippmeier:Guiding,McDonell:acc-optim}.

% \subsection{Sequence fusion}
% \TODO{Redo? Even include? Update with stuff from new version of paper}
% \TODO{Sequence fusion is fairly simple. Description doesn't need to be long.}


% Similarly, it is important that we remove intermediate structures from a sequence
% computation. Recall that a sequence computation of type @Seq [Array sh e]@
% consists of many stream parallel array computations of type @Acc (Array sh e)@.
% In the same way, sequence fusion consists of two components.
% First, we use a variant of stream fusion~\cite{Coutts:stream-fusion}%
% \footnote{One of the complexities of general fusion transformations, including
% stream fusion, is needing handle filtering operations, where the size of the
% result structure depends on the \emph{values} of the input structure, as well as
% its size. Our sequences do not support dropping (or skipping) elements of the
% stream, so we do not need to consider it here.}
% in order to combine the sequence computations. This exposes the array
% computations of the sequences to each other, allowing us to then apply an
% improved version of the Accelerate array fusion system~\cite{McDonell:acc-optim}
% to further combine the operations, which we describe next.

% We represent sequences internally as a simplified form of the concept of stream
% a from the popular fusion approach of \citet{Coutts:stream-fusion}. The key
% difference is that our sequences do not support skipping over elements. While it
% would be useful to say, perform a filter over a sequence, our system does not
% allow it due the buffering it would require. We refer you to the work on stream
% fusion for more information on how streams can be fused.


\subsection{Array fusion}

The core idea underlying the existing Accelerate array fusion
system~\cite{McDonell:acc-optim} is well known: simply represent an array by its
size and a function mapping array indices to their corresponding values. Fusion
then becomes an automatic property of the data representation. This method has
been used successfully to optimise purely functional array programs in
Repa~\cite{Keller:Repa,Lippmeier:Guiding}, although the idea of representing
arrays as functions is well known~\cite{Claessen:obsidian-expressive,Guibas:1978jh,Elliott:2003ug}.

% The
% process of array fusion then becomes one of composing these representation
% functions.

% However, the straightforward implementation of this approach requires that terms
% undergoing fusion be syntactically adjacent in the program. For example, in
% order to fuse the following program we must be able to float the definition of
% @xs@ above the outer @map@: %, otherwise the two terms will not be fused:
% %
% \begin{lstlisting}
%   map f $ let xs = use (Array ...)
%           in  map g xs
% \end{lstlisting}

However, a straightforward implementation of this approach results in a loss of
\emph{sharing}, which was a problem in early versions of
Repa~\cite{Lippmeier:Guiding}. For example, consider the following program:
%
\begin{lstlisting}
let xs = use (Array ...)
    ys = map f xs
in
zipWith g ys ys
\end{lstlisting}
%
Every access to an element @ys@ will apply the (arbitrarily expensive) function @f@
to the corresponding element in @xs@. It follows that these computations will be
done \emph{at least} twice, once for each argument in @g@, quite contrary to the
programmer's intent. In the standard Accelerate fusion system, the solution to
this problem is to not fuse terms, such as @ys@, whose results are used more than
once.

% The existing Accelerate fusion system is able to deal with such problems, where
% let bindings would otherwise inhibit fusion. However, the system does not take
% into account \emph{tuples} of arrays. This is problematic for us, since

However, this approach does not take into account what the term @ys@ actually
\emph{is}; it simply sees that @ys@ occurs twice in @zipWith@ and so refuses to
fuse it further. Consider the following example:
%
\begin{lstlisting}
let xs = use (Array ...)
    ys = (map f%$_1$% xs, map f%$_2$% xs)
in
zipWith g (fst ys) (snd ys)
\end{lstlisting}
%
Although the term @ys@ still occurs twice in the @zipWith@, we can see that the
individual components of the tuple each occur only once, and thus should still
be subject to fusion.

This lack of fusion in the regular Accelerate optimisation system is
particularly problematic for us, since we represent irregular sequences as a
pair consisting of the segment descriptor together with a vector of the array
values. Thus, in the standard Accelerate system, irregular sequences would never
fuse, severely impacting performance. We extend the Accelerate fusion system in
order to fuse the individual components of a tuple independently, which improves
the performance of both our sequence computations as well as regular Accelerate
programs.

To explain this, we'll first describe more concretely how Accelerate's prior fusion system works, then cover the fusion-through-tuples implemented as part of Accelerate sequences. To start with, consider this program here
%
\begin{lstlisting}
\xs -> map (+1) (map (*2) xs)
\end{lstlisting}
%
we fuse it by a bottom up traversal. We must first \emph{delay} @xs@ into
%
\begin{lstlisting}
(shape xs, (xs !))
\end{lstlisting}
%
Here we've created a function that accesses @xs@ paired with its extent. Proceeding up the AST, the @map (*2)@ can now be delayed by taking the delayed form of @xs@ and composing it with @*2@.
%
\begin{lstlisting}
(shape xs, (\ix -> (xs ! ix)*2))
\end{lstlisting}
%
In general,
%
\begin{lstlisting}
map f (sh, g) = (sh, f . g)
\end{lstlisting}
%
We can then do the same with the @map (+1)@, giving us a final form of
%
\begin{lstlisting}
\xs -> (shape xs, (\ix -> (xs ! ix)*2))
\end{lstlisting}

This can be brought back into array form by turning the delayed array into a generate.
%
\begin{lstlisting}
\xs -> generate (shape xs) (\ix -> (xs ! ix)*2)
\end{lstlisting}
%

This is the core idea behind Accelerate's fusion system, represent arrays in a delayed form and turn each combinator into an equivalent one operating on them. Naturally, some combinators cannot be delayed (e.g. folds and scans), so those must be left as \emph{manifest} still introducing intermediate arrays.

Another problem, as already described, is when an array is accessed more than once. In Accelerate, this can only occur with let bindings. Looking at this example here,
%
\begin{lstlisting}
let a = generate sh f
in map g a
\end{lstlisting}
%
we see that it would be beneficial to embed (inline) the binding @a@ into the body to expose fusion opportunities there. In general though, that is not always desired, as highlighted above. So, when should we embed bindings like this? There are many possible measures that could be used, but we opt for a simple one. Accelerate inlines whenever a variable is only ``used" once. By ``used" here, we mean how often the variable occurs in the body of the let binding. We will extend this definition in the next section.

There is an additional problem as well. Consider this term.
%
\begin{lstlisting}
let a = let b = scanl f z
        in map g b
in map h a
\end{lstlisting}
%
In this case, it is possible to fuse the term into
%
\begin{lstlisting}
let b = scanl f z
in map (h . g) b
\end{lstlisting}
%
However, by doing this, we've had to float the binding for @b@ outside of its previous scope. This extends the lifetime of @b@ potentially affecting the programs memory usage. In general, Accelerate will perform this let floating, but only provided it gives additional fusion opportunities.

\subsection{Fusion through tuples}

Goin back to our example.
%
\begin{lstlisting}
let xs = use (Array ...)
  ys = (map f%$_1$% xs, map f%$_2$% xs)
in
zipWith g (fst ys) (snd ys)
\end{lstlisting}
%
Under the previous system, this wouldn't fuse, despite the fact that even though the tuple was ``used" twice each component of the tuple is only accessed once.

By extending the concept of what it means for a binding to be used, we can enable terms like this to be fused. We simply need to keep a separate occurrence count for each component of a tuple.

In the above example, that would let us see that even though @t@ occurs twice in the body, each component of @t@ is only ``used" once. Hence the binding can be embedded into the body using a specialised form of embedding that immediately simplifies when it encounters tuple projection. This results in
%
\begin{lstlisting}
zipWith h (generate sh f) (generate sh' g)
\end{lstlisting}
%
which can be fused using the rules described above. If we were to embed via conventional inlining, just replacing @ys@ with its definition at all use sites, we would end up with
%
\begin{lstlisting}
zipWith h (fst (generate sh f, generate sh' g)) (snd (generate sh f, generate sh' g))
\end{lstlisting}
%
which, while still fusing after further simplification, is much larger than the original term. In general, simple inlining can cause an explosion in the size of the term, slowing down subsequent optimisation.

Let's now look at a more complicated example:
%
\begin{lstlisting}
let t = (generate sh f, scanl g z arr, map h arr)
in ( zipWith h (prj 1 t) (prj 3 t)
   , zipWith k (prj 3 t) (prj 2 t) )
\end{lstlisting}
%
This brings up the question of what we should do when some components of a tuple are delayable (the @generate) and are only used once, but others are either not delayable (the @scanl) or occur multiple times (the @map@)? In this case we inline the first component of @t@ but leave the second and 3rd components bound.
%
\begin{lstlisting}
let t' = (scanl g 0 arr, map h arr)
in ( zipWith h (generate sh f) (prj 2 t')
   , zipWith k (prj 2 t') (prj 1 t) )
\end{lstlisting}
%
In general, when we encounter a let binding of a tuple like this, we split the tuple into two tuples, one containing all the components that are delayable and only occur once, the other containing all the components that should remain let-bound. The delayable tuple gets inlined using the specialised inline, the bound one remains let-bound.

Of course, like above, we also have to consider nested bindings:
%
\begin{lstlisting}
let a = let b = scanr j 1 arr
        in (map f b, scanl g 0 b)
in zipWith h (fst a) (snd a)
\end{lstlisting}
%
This gets transformed into
%
\begin{lstlisting}
let b = scanr j 1 arr in
let a = scanl g 0 b
in zipWith h (map f b) a
\end{lstlisting}
%
Here we are floating @b@ out, possibly extending its lifetime. However, by doing this, we are able to fuse the first component of @a@. As above, we consider that the benefit of fusion outweighs the cost of let floating.

% \rob{Maybe I should mention the properties of the accelerate language that make this optimisation useful? i.e. tuple constructors can only occur in a handful of places. Also, it may not be obvious from what I've described here that this also works for nested tuples}

While initially these fusion rules relating to tuples may seem to only work in specific situations, it is worth noting that they in fact stop tuples being a barrier to fusion in all cases: simply due to the restricted nature of the Accelerate language. The two key properties that make this happen are that:
  (1) Tuple constructors can only occur in a few places. In the argument to tuple projections, in let bindings and in other tuple constructors. None of the combinators take tuples as arguments.
  (2) Unnecessary indirection of the form @let v_0 = v_1 in c[v_0]@ is eliminated during fusion as well.

We've already shown how tuple constructors in let bindings can be fused. The case where a tuple constructor is passed to tuple projection is trivial. The only remaining case is tuple constructors in other tuple constructors. For example,
%
\begin{lstlisting}
let x = (a, (b, c, d))
in ...
\end{lstlisting}
%
Supposing that @b@ is delayable and is only ``used" once by the body, but that a, c and d need to remain bound, we have to be careful. We capture the exact usage pattern of such tuples by maintaining usage information in the following representation:
%
\begin{lstlisting}
data Uses a where
  UsesArray :: Int                         -- How many times the contents of the array is accessed
            -> Int                         -- How many times the shape of the array is used
            -> Uses (Array sh e)
  UsesTuple :: (Uses t_0, ..., Uses t_n)
            -> Uses (t_0,..,t_n)
\end{lstlisting}
%
As a result, we capture how every component or sub-component of a tuple is accessed by the body of the binding.

When we inline with nested tuples, we similarly split each tuple into two smaller tuples. In the above example, we get
%
\begin{lstlisting}
let x = (a, (c, d))
in ...
\end{lstlisting}
%
with @b@ now fused into the body.


\section{Scheduling}
\label{sec:scheduling}

We are still left with the problem of deciding how many elements of a sequence
we should compute at any one time. This problem was not the focus of this work, but is still necessary to enable efficient execution. One approach, as described in \citet{Madsen:2015}, is to analyse whole sequence computations and chose a static number based on an analysis of their parallel
degree. While this works for regular computations, in the presence of
irregularity, where the size of individual elements may vary greatly, there is
in general no good fixed static size. Hence, we use a dynamic scheduling
approach, constantly adjusting the number of elements of the sequence to execute
at once.
% This adjustment happens at each step of the whole sequence computation, not
% between individual operations.
For example, consider the following sequence computation:
%
\begin{lstlisting}
consume (elements (mapSeq f (mapSeq g xs)))
\end{lstlisting}
%
% \begin{lstlisting}
% consume . elements $ zipWithSeq f (mapSeq g xs) ys
% \end{lstlisting}
% \gck{I don't understand which changing the sequence size here wold be a problem - if there was a zipWith, but why with a map?}
% \rob{You're right. This example can be scheduled in that way. Maybe something like what's above now?}
% \gck{Yes, I guess. Rob - can you fix the description? I'm not quite sure how it works with the chunking if two sequences are involved, and exactly where it would clash}
% \gck{Much better }
% \rob{I've actually gone back to the old example and changed the point we were trying to make slightly. I was really trying to get at why multi-rate scheduling doesn't work for sequences with content of unknown size. Does this make sense?}
% Here, @consume :: Seq arr -> Acc arr @ marks the boundary of the sequence
% computation with the @Acc@ computation it is embedded in.
Executing a step of the sequence computation consists of (1) computing some
chunk of the input @xs@; (2) apply the lifted version of @g@ to the chunk; (3) apply
the lifted version of @f@ to that result; and (4) store the result. Now, suppose
that @g@$^\uparrow$ achieves best performance when processing $N$ elements at a
time, but @f@$^\uparrow$ prefers a size of $2N$ for best performance. We could
conceivably compute two $N$-sized chunks with @g@$^\uparrow$, then combine these
into a $2N$-sized chunk for @f@$^\uparrow$. However, even though the size of the
chunk may be known, the actual size of the data in each chunk (of an irregular
computation) is not. For this reason, any sort of multi-rate scheduling requires
either considerable copying of intermediates or unbounded buffers. We avoid this
issue by choosing to only adjust the chunk size after each complete step of the
sequence computation.

We have two competing considerations for determining how many elements we should
process in a step of a sequence computation: (1) we want to maximise processor
utilisation, to ensure that all processing elements are busy; and (2) minimise
the amount of time taken for each element of the sequence, which also acts as an
approximate measure to minimising overall system resource requirements, such as
memory usage. Sequence computations initially execute a single element of the
sequence, then subsequently select a chunk size for the next step based on the
following strategy:
%
\begin{itemize}
  \item If the overall processor utilisation (time spent executing sequence
    computations compared to the elapsed wall time) is below a target threshold
    (80\%), increase the chunk size.
    This is particularly important when the computation is bootstrapping from
    small chunk sizes and the elements of the sequence are small.

  \item Once the processor is sufficiently utilised, we continue to increase the
    chunk size only if it decreases the average time per element. If the time
    per element instead increases, decrease the chunk size. Otherwise, maintain
    the chunk size.

    In particular, the ability to decrease the chunk size is
    necessary to deal sequences containing elements that grow in size.
    Additionally, this acts as a proxy to minimise usage of other system
    resource, such as memory.

    If the processor is sufficiently utilised; i.e. above the minimum
    threshold, we continue to increase the chunk size only if it yields a decrease
    in the amount of time, on average, to process each element in the chunk. If
    no the time taken does no decrease, we decrease the chunk size. This last
    part is necessary for sequences containing elements that grow in size.

\end{itemize}
%
As practical considerations, our implementation uses weighted moving averages of
sampled variables, and increases the chunk size at twice the rate that we
decrease it, which reduces warm-up time and biases towards ensuring the
processors remains saturated.

\endinput


% Following vectorisation, we represent a sequence computation as series of producers followed by consumers. Each producer consists of a limit, how many elements in total it will produce (Nothing if it is infinite); a state transformation function, a function that will take the starting index of the current sequence chunk as well as its size and the previous state and produce a new state and a new element of the sequence; and an initial state.

% \begin{lstlisting}
% data Seq env a where
%   Producer :: Producer b
%            -> Seq (env,b) a
%            -> Seq env a
%   Consumer :: Consumer env a
%            -> Seq env a

% data Producer env a where
%   ProduceAccum :: Maybe Int                           -- The limit
%                -> Afun env ((Int,Int) -> s -> (s, a)) -- State function
%                -> Ref s                               -- Previous state
%                -> Producer env a

% data Consumer env a where
%   Last :: Var env a
%        -> Consumer env a

%   Tuple :: (Seq env a_0, ..., Seq env a_n)
%         -> Consumer env (a_0,...,a_n)
% \end{lstlisting}

% It is then possible to execute a single step of a sequence computation with @stepSeq@.

% \begin{lstlisting}
% stepSeq :: Int -> Int -> env -> a -> Seq a -> IO (Maybe a)
% stepSeq index size env prev (Producer (ProduceAccum limit f stateRef) seq) =
%   if fromMaybe True ((index <) <$> limit) then do
%     let size' = min size (limit - index)
%     state <- read stateRef
%     (state', result) <- execAfun f (index, size') state
%     write stateRef state'
%     stepSeq index size' (env,result) prev seq
%   else return Nothing -- Sequence is done. Whatever value was returned from the last step is the result.
% stepSeq _ _ env _ (Consumer (Last v))
%   = return (Just (prj v env)) -- The sequence computation has yielded a value.
% stepSeq index size env prev (Consumer (Tuple (a_0, ..., a_n))) = do
%   a_0' <- stepSeq index size env (prj 0 a) a_0
%   ...
%   a_n' <- stepSeq index size env (prj n a) a_n
%   -- If all a's are Nothing then return Nothing. Otherwise, replace all Nothings with the previous values and return the new tuple.
% \end{lstlisting}

% It is important to note that by stepping the sequence in this way we can support sequences of different lengths. In the case of tuples, if we stopped computing a tuple whenever one of its components could not be stepped then we would be unable to support sequence computations like,

% \begin{lstlisting}
% lift (sumSeq xs, sumSeq ys) :: Seq (Scalar Int, Scalar Int)
% \end{lstlisting}

% if xs and ys were different lengths.

% % \TODO{Explain how these sequence computations can be fused down to one ProduceAccum}

% While this lets us execute sequence computations, it does not take into account \emph{fusion}. For example this sequence, where we assume xs is a regular sequence,

% \begin{lstlisting}
% sumSeq $ mapSeq (map (+1)) xs
% \end{lstlisting}

% would, after vectorisation, be converted to

% \begin{lstlisting}
% ProduceAccum ... -- However xs is computed
% `Producer`
% ProduceAccum Nothing (\_ () -> ((),map (+1) v%$_0$%)) -- v%$_0$% is the producer bound above
% `Producer`
% ProduceAccum Nothing (\_ s -> let s' = s + sum v%$_0$% in (s',s'))
% `Producer`
% Consumer (Last v%$_0$%)
% \end{lstlisting}

% Here we see an obvious problem. We're creating a whole new sequence chunk just to perform the @map (+1)@. We want this to be fused, and fortunately accelerate already supports array fusion. So supporting sequence fusion is just a matter of combining producers such that array fusion can happen.

% For this example we merge the two producers into one like so

% \begin{lstlisting}
% ProduceAccum ... -- However xs is computed
% `Producer`
% ProduceAccum Nothing (\_ ((),s) -> let sa = ((),map (+1) v_0)
%                                        s' = (s + sum (snd sa))
%                                    in (s',s')
% `Producer`
% Consumer (Last v_0)
% \end{lstlisting}

% This new producer now contains an array function that can be fused by accelerate's array fusion.

\subsection{Array fusion}
% \TODO{Reference previous paper and talk about simple extension to support tuples of producers.}

Accelerate's existing array fusion optimisation is described in~\cite{McDonell:acc-optim}. We will first give a brief overview of a simplified version of the optimisation. For a more detailed description, we refer to the paper.

Accelerate fuses arrays be representing them in a delayed form, consisting of the shape of an array and a function that for every index produces an element.
%
\begin{lstlisting}
Array sh e ~> (sh, (sh -> a))
\end{lstlisting}
