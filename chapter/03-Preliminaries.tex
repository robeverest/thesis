\chapter{Preliminaries}

\section{Type preserving transformations}
\TODO{Describe generalisation of McBride's technique}

\section{Using foreign libraries}
\label{sec:foreign-import}

\TODO{Revise for flow and recent API changes}

% http://www.haskell.org/onlinereport/haskell2010/haskellch8.html#x15-1490008

Accelerate is a high-level language framework capturing idioms suitable for massively parallel GPU architectures, without requiring the expert knowledge needed to achieve good performance at the level of CUDA. However, there are existing highly optimised CUDA libraries, for example, for high performance linear algebra and fast Fourier transforms. For Accelerate to be practically useful, we need to provide a means to use those libraries. Moreover, access to native CUDA code also provides a developer the opportunity to drop down to raw CUDA C in those parts of an application where the code generated by Accelerate is not sufficiently efficient. We achieve access to CUDA libraries and native CUDA components with the  \emph{Accelerate Foreign Function Interface} (or FFI).

The Accelerate FFI is a two-way street: (1) it enables calling native CUDA C code from embedded Accelerate computations and (2) it facilitates calling Accelerate computations from non-Haskell code. Overall, a developer can implement an application in a mixture of Accelerate and other languages in a manner that the source code is portable across multiple Accelerate backends.

Given that Accelerate is embedded in Haskell, it might seem  that Haskell's standard FFI should be sufficient to enable interoperability with foreign code. Unfortunately, this is not the case. With Haskell's standard FFI, we can call C functions that in turn invoke GPU computations from Haskell host code. However, we want to call GPU computations from within embedded Accelerate code and pass data structures located in GPU memory directly to native CUDA code and vice versa. The latter is crucial, as transferring data from CPU memory to GPU memory and back is very expensive.

\subsection{Importing foreign functions}

Calling foreign code in an embedded Accelerate computation requires two steps: (1) the foreign function must be made accessible to the host Haskell program and (2) the foreign function must be lifted into an Accelerate computation to be available to embedded code. For the first step, we use the standard Haskell FFI. The second step requires an extension to Accelerate.

As a concrete example, let us use the vector dot product of the highly optimised \emph{CUDA Basic Linear Algebra Subprograms (CUBLAS)} library~\cite{cublas}. This CUBLAS function is called @cublasSDot()@; it computes the vector dot product of two arrays of 32-bit floating point values. To access it from Haskell, we use this Haskell FFI import declaration:
%
\begin{lstlisting}[language=haskell]
foreign import ccall "cublas_v2.h cublasSdot_v2" cublasSdot
  :: Handle
  -> Int                                -- Number of array elements
  -> DevicePtr Float -> Int             -- The two input arrays, and...
  -> DevicePtr Float -> Int             -- ...element stride
  -> DevicePtr Float                    -- Result array
  -> IO ()
\end{lstlisting}
%
The @Handle@ argument is required by the foreign library and created on
initialisation. The @DevicePtr@ arguments are pointers into GPU memory. As mentioned before, the primary aim of the Accelerate FFI is to ensure that we do not unnecessarily transfer data between GPU and CPU memory.

To manage device pointers, the Accelerate FFI provides a GPU memory allocation function @allocateArray@ and a function @devicePtrsOfArray@ to extract the device pointers of an Accelerate array. We can use these functions to invoke @cublasSdot@ with GPU-side data:
%
\begin{lstlisting}[language=haskell]
dotp_cublas :: Handle
            -> (Vector Float, Vector Float)
            -> CIO (Scalar Float)
dotp_cublas handle (xs, ys) = do
  let n  = arraySize (arrayShape xs)    -- number of input elements
  result    <- allocateArray Z          -- allocate a new Scalar array
  ((),xptr) <- devicePtrsOfArray xs     -- get device memory pointers
  ((),yptr) <- devicePtrsOfArray ys
  ((),rptr) <- devicePtrsOfArray result
  liftIO $ cublasSdot handle n xptr 1 yptr 1 rptr
  return result
\end{lstlisting}
%
The result of @devicePtrsOfArray@ is a nested tuple of pointers, as we represent arrays of tuples as tuples of arrays; hence, we can have multiple CUDA arrays for one Accelerate array. In the above example, there is only one, though. The @CIO@ monad is simply the @IO@ monad enriched with some information used by the CUDA backend to manage devices, memory, and caches.

\subsection{Executing foreign functions with Accelerate}

The function @dotp_cublas@ invokes native CUDA code in such a manner that it directly uses arrays in GPU memory. This leaves us with two challenges: (1) we need to enable calling functions, such as @dotp_cublas@, in embedded code and (2) we need to account for Accelerate supporting multiple backends, while Accelerate programs should be portable across backends.

To discuss these issues, we need to briefly recap some of the Accelerate internals described in~\cite{Chakravarty:acc-cuda}. Accelerate reifies embedded programs into an abstract syntax tree (AST) encoded as a generalised abstract data type (GADT) to track types of the embedded language in the host language --- i.e., the AST can only represent well-typed embedded programs. Accelerate compiles fused collections of array operations into GPU kernels and orchestrates the execution of those kernels CPU-side by a tree traversal of the AST.

Returning to the two remaining challenges, we address the challenge of enabling calling functions, such as @dotp_cublas@, by extending the AST with a new node type @Aforeign@ representing foreign calls. One instance of an @Aforeign@ node encodes the code for one backend, but it also contains a fallback implementation in case a different backend is being used. The AST data constructor is defined as follows:
%
\begin{lstlisting}[language=haskell]
Aforeign :: (Arrays as, Arrays bs, Foreign f)
         => f as bs                     -- foreign function
         -> (Acc as -> Acc bs)          -- fallback implementation
         -> Acc as                      -- input array
         -> Acc bs
\end{lstlisting}
%
When the tree walk during code execution encounters an @Aforeign@ AST node, it dynamically checks whether it can execute the foreign function. If it can't, it instead executes the fallback implementation. A fallback implementation might be another @Aforeign@ node with native code for a different backend (e.g., for OpenCL instead of CUDA), or it can simply be a vanilla Accelerate implementation of the same functionality that is provided by the foreign code. With a cascade of @Aforeign@ nodes, we can provide an optimised native implementation of a function for a range of backends and still maintain a vanilla Accelerate version of the same functionality for execution in the Accelerate interpreter.

The dynamic check for the suitability of a foreign function is facilitated by the class constraint @Foreign f@ in the context of @Aforeign@. The class @Foreign@ is a subclass of @Typeable@ with instances for data types that represent foreign functions for specific backends. For the CUDA backend, we have the following:
%
{\small
\begin{lstlisting}
  class Typeable2 f => Foreign f where ...
  instance Foreign CUDAForeignAcc where ...
  data CUDAForeignAcc as bs where
    CUDAForeignAcc :: as -> CIO bs
\end{lstlisting}%
}\noindent
@CUDAForeignAcc@ wraps calls to foreign CUDA code executed in the @CIO@ monad. When the CUDA backend encounters an AST node  @Aforeign foreignFun alt arg@, it attempts to @cast@\footnote{See Haskell's @Data.Typeable@ library for details on @cast@.} the value of @foreignFun@ to type @CUDAForeignAcc as bs@. If that @cast@ succeeds, it can unwrap the @CUDAForeignAcc@ and invoke the function it contains. Otherwise, it needs to execute the alternative implementation @alt@.

Finally, we can define an embedded vector dot product that uses CUBLAS when possible and, otherwise, falls back to the version defined in Section~\ref{sec:background-accelerate}:
%
{\small
\begin{lstlisting}
  dotp' :: Acc (Vector Float) -> Acc (Vector Float)
        -> Acc (Scalar Float)
  dotp' xs ys = Aforeign (CUDAForeignAcc (dotp_cublas handle))
                         (uncurry dotp)
                         (lift (xs, ys))
\end{lstlisting}
}\noindent%
%
Foreign calls are not curried; hence, they only have got one argument, which is an instance of the class @Arrays@ of tuples of Accelerate arrays.

\subsection{Embedding foreign scalar functions}

So far, we discussed the use of foreign array computations from Accelerate. However, we also wish to be able to use foreign scalar operations in embedded array computations. For example, CUDA provides fused floating-point multiply-add intrinsics with a variety of rounding modes.

We import foreign scalar functions similarly to foreign array computations. In particular, the AST type @Exp@ for scalar embedded computations includes a data constructor @Foreign@ that serves the same purpose as @Aforeign@ for @Acc@:
%
{\small
\begin{lstlisting}
  Foreign :: (Elt x, Elt y, Foreign f)
          => f x y -> (Exp x -> Exp y) -> Exp x -> Exp y
\end{lstlisting}
}\noindent%
%
Where we used @CUDAForeignAcc@ to wrap CUDA array computations for use with @Aforeign@, we use @CUDAForeignExp@ to wrap scalar CUDA functions for use with @Foreign@. However, instead of wrapping a Haskell FFI call, the scalar case simply encodes the textual representation of the CUDA function in CUDA code. As discussed in Section~\ref{sec:background-accelerate}, scalar code is used to instantiate skeleton templates. The skeleton code is a template for CUDA code; so, a Haskell function invocation wouldn't be appropriate. As in the array case, functions are uncurried, but in the scalar case, they can only return a single scalar argument:
%
{\small
\begin{lstlisting}
  data CUDAForeignExp x y where
    CUDAForeignExp :: IsScalar y
                   => [String] -> String -> CUDAForeignExp x y
\end{lstlisting}%
}\noindent
%
The first argument is a list of header files that need to be included when compiling an instantiated skeleton template including this specific foreign function.

Overall, we define a foreign function based on CUDA's explicitly fused floating-point multiply-add intrinsics as follows (using IEEE rounding towards zero):
%
{\small
\begin{lstlisting}
  fmaf :: Exp Float -> Exp Float -> Exp Float -> Exp Float
  fmaf x y z = Foreign (CUDAForeignExp [] "__fmaf_rz")
                       (\v -> let (x,y,z) = unlift v in x * y + z)
                       (lift (x, y, z))
\end{lstlisting}
}

\section{Embedding embedded programs}
\label{sec:foreign-export}

Accelerate simplifies writing GPU code as it obviates the need to understand most low-level details of GPU programming. Hence, we would like to use Accelerate from other languages. As with importing foreign code into Accelerate, the foreign export functionality of the standard Haskell FFI is not sufficient for efficiently using Accelerate from languages, such as C. In the following, we describe how the Accelerate FFI supports exporting Accelerate code as standard C calls.

% \trev{some observations on the API decisions which I didn't really grok. I'll
% make some reasonable assumptions along these lines as I go over the text.}
% \begin{itemize}
%     \item @fromDeviceContext@ is a bit broken, because it is possible to supply
%         a device id $\neq$ context's device id. Moreover you can recover the
%         device ID from a context, so it's not required. I do think however we
%         should take the backend's approach and have versions that choose a
%         context and ones that supply it: @accInit@ and @accInitWith@. \rob{Yes,
%         that makes sense.}
%
%
%     \item if @accelerateInit@ is going to call @hs_init()@, you've restricted
%         yourself to a single accelerate execution context so you might as well
%         make the accelerate handle some global value and never deal with it
%         again. I don't think it makes sense to have more than one @AccHandle@
%         anyway. Stash it in @AccFFI.h@? Alternatively, rename things and don't
%         call @hs_init@. \rob{While accelerateInit can only be called once,
%         accelerateCreate can be called multiple times allowing support for
%         multiple devices, contexts, etc. I added accelerateInit as a convenience
%         function for the most common scenario of only using one device.}
%         \trev{aah, I didn't see the create function there.}
%
%     \item I think the compile/run functions should be combined, which matches
%         the behaviour of run1; it's a bit boring to have to remember to compile
%         a function first (perhaps not for a C programmer?). A @static@ stable
%         pointer in the execution function? \rob{It wouldn't be hard to combine
%         to combine the two. I just wasn't sure whether I should as it didn't
%         seem very C like. However, I think you said OpenCV compiles a function
%         the first time it is executed? If they can get away with it, then I
%         suppose we can as well.}
% \end{itemize}

%\rob{Perhaps we should change this section title? I'm not sure it really matches what we describe.}

\subsection{Exporting Accelerate programs}

To export Accelerate functions as C functions, we make use of Template Haskell~\cite{Sheard:2002template}. For example, we might export our Accelerate dot product:
%
{\small
\begin{lstlisting}
  dotp :: Acc (Vector Float, Vector Float) -> Acc (Scalar Float)
  dotp = uncurry $ \xs ys -> fold (+) 0 (zipWith (*) xs ys)

  exportAfun 'dotp "dotp_compile"
\end{lstlisting}
}\noindent%
%
The function @exportAfun@ is defined in Template Haskell and takes the name of an Accelerate function, here @dotp@, as an argument. It generates the necessary export declarations by inspecting the properties of the name it has been passed, such as its type.

Compiling a module that exports Accelerate computations in this way (say,
@M.hs@) generates the additional file @M_stub.h@ containing the C prototype for
the foreign exported function. For the dot product example, this header contains:
%
{\small
\begin{lstlisting}
  #include "HsFFI.h"
  extern AccProgram dotp_compile(AccContext a1);
\end{lstlisting}
}\noindent%
%
A C program needs to include this header to call the Accelerate dot product.


\subsection{Running embedded Accelerate programs}

One of the functions to execute an Accelerate computation in Haskell is:
%
{\small
\begin{lstlisting}
  run1In :: (Arrays as, Arrays bs)
         => Context -> (Acc as -> Acc bs) -> as -> bs
\end{lstlisting}
}\noindent%
%
This function comprises two phases: (1) program optimisation and instantiation of skeleton templates of its second argument and (2) execution of the compiled code in a given CUDA context (first argument). The implementation of @run1In@ is structured such that, partially applying it to only its first and second argument, yields a new function of type @as -> bs@, where Phase (1) has been executed already --- in other words, it precompiles the Accelerate code. Repeated application of this function of type @as -> bs@ executes the CUDA code without any of the overheads associated with just-in-time compilation.

The Accelerate export API retains the ability to precompile Accelerate code. The C function provided by @exportAfun@ compiles the Accelerate code, returning a reference to the compiled code. Then, in a second step, @runProgram@ marshals input arrays, executes the compiled program, and marshals output arrays:
%
{\small
\begin{lstlisting}
  OutputArray   out;
  InputArray    in[2]   = { ... };
  AccProgram    dotp    = dotp_compile( context );

  runProgram( dotp, in, &out );
\end{lstlisting}
}\noindent%
%
The function @dotp_compile@ was generated by @exportAfun 'dotp "dotp_compile"@.

\subsection{Marshalling input and output arrays}

Accelerate uses a non-parametric representation of multi-dimensional arrays: an array of tuples is represented as a tuple of arrays. The type @InputArray@ follows this convention. It is a C struct comprising an array of integers indicating the extent of the array in each dimension together with an array of pointers to each underlying GPU array of primitive data.
%
{\small
\begin{lstlisting}
  typedef struct { int* shape; void** adata; } InputArray;
\end{lstlisting}
}

@OutputArray@ includes an extra field, a stable pointer, that maintains a reference to the associated Haskell-side @Array@. This keeps the array from being garbage collected until the @OutputArray@ is explicitly released with @freeOutput@.
%
{\small
\begin{lstlisting}
  typedef struct { int* shape; void** adata;
                   HsStablePtr stable_ptr; } OutputArray;
\end{lstlisting}
}


\section{GPU aware garbage collection}
\TODO{Describe LRU gpu cache. Need to come up with a better motivating example.}