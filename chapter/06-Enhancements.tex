\chapter{Enhancements}
\label{chap:enhancements}

To further complement the sequences extension and flattening transformation described in this work, in this chapter we introduce an additional two extensions to the Accelerate language and runtime system. They significantly improve the practical value of the Accelerate framework, both for programs using sequences and those that do not.

To take advantage of already highly-optimised CUDA libraries, we describe a foreign function interface (FFI) supporting both importing foreign functions into Accelerate, and exporting Accelerate programs for access via conventional CUDA C/C++ programs (Section~\ref{sec:foreign}). This is the first, to our knowledge, instance of an Embedded Domain Specific Language (EDSL) having a an FFI.

To allow for programs that depend on more data than is available in GPU memory, but do not depend on it all at once, we also describe a GPU aware garbage collector (Section~\ref{sec:gpu-gc}).

% \section{Type preserving transformations}
% \TODO{Describe generalisation of McBride's technique}

\section{Foreign Function Interface}
\label{sec:foreign}

% http://www.haskell.org/onlinereport/haskell2010/haskellch8.html#x15-1490008

The main advantage of Accelerate has over languages is that programs can be written without requiring the expert knowledge needed to achieve good performance. This is particularly true for programming GPU architectures, as that requires specific knowledge of the architecture, as highlight in Section~\ref{sec:cuda}. However, there are existing highly optimised libraries, for example, for high performance linear algebra and fast Fourier transforms. For Accelerate to be practically useful, we need to provide a means to use those libraries. Moreover, access to native code also provides a developer the opportunity to drop down to low level C or CUDA C in those parts of an application where the code generated by Accelerate is not sufficiently efficient. We achieve access to CUDA libraries and native CUDA components with the \emph{Accelerate Foreign Function Interface} (or FFI).

The Accelerate FFI works in both directions: (1) it enables calling native CUDA C code from embedded Accelerate computations and (2) it facilitates calling Accelerate computations from non-Haskell code. Overall, a developer can implement an application in a mixture of Accelerate and other languages in a manner that the source code is portable across multiple Accelerate backends.

Given that Accelerate is embedded in Haskell, it might seem  that Haskell's standard FFI should be sufficient to enable interoperability with foreign code. Unfortunately, this is not the case. With Haskell's standard FFI, we can call C functions from Haskell host code. However, we want to call functions from within embedded Accelerate code and, in the case of CUDA, pass data structures located in GPU memory directly to native CUDA code and vice versa. The latter is crucial, as transferring data from CPU memory to GPU memory and back is very expensive.

\subsection{Importing foreign functions}
\label{sec:foreign-import}

Calling foreign code in an embedded Accelerate computation requires two steps: (1) the foreign function must be made accessible to the host Haskell program and (2) the foreign function must be lifted into an Accelerate computation to be available to embedded code. For the first step, we use the standard Haskell FFI. The second step requires an extension to Accelerate. We will focus on interfacing with CUDA, as it also requires us to consider data transfer. Interfacing with CPU-based libraries is much the same, but somewhat simpler as they work in the same memory space.

As a concrete example, let us use the vector dot product of the highly optimised \emph{CUDA Basic Linear Algebra Subprograms (CUBLAS)} library~\cite{cublas}. This CUBLAS function is called @cublasSDot()@; it computes the vector dot product of two arrays of 32-bit floating point values. To access it from Haskell, we use this Haskell FFI import declaration:
%
\begin{lstlisting}[language=haskell]
foreign import ccall "cublas_v2.h cublasSdot_v2" cublasSdot
  :: Handle
  -> Int                                -- Number of array elements
  -> DevicePtr Float -> Int             -- The two input arrays, and...
  -> DevicePtr Float -> Int             -- ...element stride
  -> DevicePtr Float                    -- Result array
  -> IO ()
\end{lstlisting}
%
The @Handle@ argument is required by the foreign library and created on
initialisation. The @DevicePtr@ arguments are pointers into GPU memory. As mentioned before, one of the primary aims of the Accelerate FFI is to ensure that we do not unnecessarily transfer data between GPU and CPU memory.

To manage device pointers, the Accelerate FFI provides a GPU memory allocation function @allocateArray@ and a function @withDevicePtr@ to perform a computation that depends on a device pointer of an Accelerate array. We can use these functions to invoke @cublasSdot@ with GPU-side data:
%
\begin{lstlisting}[language=haskell]
dotp_cublas :: Handle
            -> (Vector Float, Vector Float)
            -> LLVM PTX (Scalar Float)
dotp_cublas handle (xs, ys) = do
  let n  = arraySize (arrayShape xs)   -- number of input elements
  result <- allocateArray Z            -- allocate a new Scalar array
  withDevicePtr xs         $ \xptr ->  -- get device memory pointers
    withDevicePtr ys       $ \yptr ->
      withDevicePtr result $ \rptr ->
        liftIO $ cublasSdot handle n xptr 1 yptr 1 rptr
  return result
\end{lstlisting}
%
The @LLVM PTX@ monad is simply the @IO@ monad enriched with some information used by the PTX backend to manage devices, memory, and caches.

\subsection{Executing foreign functions with Accelerate}

The function @dotp_cublas@ invokes native CUDA code in such a manner that it directly uses arrays in GPU memory. This leaves us with two challenges: (1) we need to enable calling functions, such as @dotp_cublas@, in embedded code and (2) we need to account for Accelerate supporting multiple backends, while Accelerate programs should be portable across backends.

We address this challenge by extending the AST with a new node type @Aforeign@ representing foreign calls. One instance of an @Aforeign@ node encodes the code for one backend, but it also contains a fallback implementation in case a different backend is being used. The AST data constructor is defined as follows:
%
\begin{lstlisting}
  Aforeign    :: (Arrays as, Arrays bs, Foreign asm)
              => asm                   (as -> bs)    -- The foreign function for a given backend
              -> PreAfun      acc      (as -> bs)    -- Fallback implementation(s)
              -> acc              aenv as            -- Arguments to the function
              -> PreOpenAcc   acc aenv bs
\end{lstlisting}
%
When the tree walk during code execution encounters an @Aforeign@ AST node, it dynamically checks whether it can execute the foreign function. If it can't, it instead executes the fallback implementation. A fallback implementation might be another @Aforeign@ node with native code for a different backend (e.g., for CPU instead of PTX), or it can simply be a vanilla Accelerate implementation of the same functionality that is provided by the foreign code. With a cascade of @Aforeign@ nodes, we can provide an optimised native implementation of a function for a range of backends and still maintain a vanilla Accelerate version of the same functionality for execution in the Accelerate interpreter.

The dynamic check for the suitability of a foreign function is facilitated by the class constraint @Foreign f@ in the context of @Aforeign@. The class @Foreign@ is a subclass of @Typeable@ with instances for data types that represent foreign functions for specific backends. For the CUDA backend, we have the following:
%
\begin{lstlisting}
  class Typeable2 f => Foreign f where ...
  instance Foreign ForeignAcc where ...
  data ForeignAcc f where
    ForeignAcc :: String
               -> (Stream -> a -> LLVM PTX b)
               -> ForeignAcc (a -> b)
\end{lstlisting}%
%
@ForeignAcc@ wraps calls to foreign CUDA code executed in the @LLVM PTX@ monad. When the CUDA backend encounters an AST node  @Aforeign foreignFun alt arg@, it attempts to @cast@\footnote{See Haskell's @Data.Typeable@ library for details on @cast@.} the value of @foreignFun@ to type @ForeignAcc f@. If that @cast@ succeeds, it can unwrap the @CUDAForeignAcc@ and invoke the function it contains. Otherwise, it needs to execute the alternative implementation @alt@.

Finally, we can define an embedded vector dot product that uses CUBLAS when possible and, otherwise, falls back to the version defined in Section~\ref{sec:background-accelerate}:
%
\begin{lstlisting}
  dotp' :: Acc (Vector Float) -> Acc (Vector Float)
        -> Acc (Scalar Float)
  dotp' xs ys = Aforeign (CUDAForeignAcc (dotp_cublas handle))
                         (uncurry dotp)
                         (lift (xs, ys))
\end{lstlisting}%
%
Foreign calls are not curried; hence, they only have got one argument, which is an instance of the class @Arrays@ of tuples of Accelerate arrays.

% \subsection{Embedding foreign scalar functions}

% So far, we discussed the use of foreign array computations from Accelerate. However, we also wish to be able to use foreign scalar operations in embedded array computations. For example, CUDA provides fused floating-point multiply-add intrinsics with a variety of rounding modes.

% We import foreign scalar functions similarly to foreign array computations. In particular, the AST type @Exp@ for scalar embedded computations includes a data constructor @Foreign@ that serves the same purpose as @Aforeign@ for @Acc@:
% %
% \begin{lstlisting}
%   Foreign :: (Elt x, Elt y, Foreign f)
%           => f x y -> (Exp x -> Exp y) -> Exp x -> Exp y
% \end{lstlisting}%
% %
% Where we used @CUDAForeignAcc@ to wrap CUDA array computations for use with @Aforeign@, we use @CUDAForeignExp@ to wrap scalar CUDA functions for use with @Foreign@. However, instead of wrapping a Haskell FFI call, the scalar case simply encodes the textual representation of the CUDA function in CUDA code. As discussed in Section~\ref{sec:background-accelerate}, scalar code is used to instantiate skeleton templates. The skeleton code is a template for CUDA code; so, a Haskell function invocation wouldn't be appropriate. As in the array case, functions are uncurried, but in the scalar case, they can only return a single scalar argument:
% %
% \begin{lstlisting}
%   data CUDAForeignExp x y where
%     CUDAForeignExp :: IsScalar y
%                    => [String] -> String -> CUDAForeignExp x y
% \end{lstlisting}%
% %
% The first argument is a list of header files that need to be included when compiling an instantiated skeleton template including this specific foreign function.

% Overall, we define a foreign function based on CUDA's explicitly fused floating-point multiply-add intrinsics as follows (using IEEE rounding towards zero):
% %
% \begin{lstlisting}
%   fmaf :: Exp Float -> Exp Float -> Exp Float -> Exp Float
%   fmaf x y z = Foreign (CUDAForeignExp [] "__fmaf_rz")
%                        (\v -> let (x,y,z) = unlift v in x * y + z)
%                        (lift (x, y, z))
% \end{lstlisting}
%

\subsection{Exporting functions}
\label{sec:foreign-export}

Accelerate simplifies writing high performance code as it lessens the need to understand most low-level details of, higher level programming. Hence, we would like to use Accelerate from other languages. As with importing foreign code into Accelerate, the foreign export functionality of the standard Haskell FFI is not sufficient for efficiently using Accelerate from languages, such as C. In the following, we describe how the Accelerate FFI supports exporting Accelerate code as standard C calls.

% \trev{some observations on the API decisions which I didn't really grok. I'll
% make some reasonable assumptions along these lines as I go over the text.}
% \begin{itemize}
%     \item @fromDeviceContext@ is a bit broken, because it is possible to supply
%         a device id $\neq$ context's device id. Moreover you can recover the
%         device ID from a context, so it's not required. I do think however we
%         should take the backend's approach and have versions that choose a
%         context and ones that supply it: @accInit@ and @accInitWith@. \rob{Yes,
%         that makes sense.}
%
%
%     \item if @accelerateInit@ is going to call @hs_init()@, you've restricted
%         yourself to a single accelerate execution context so you might as well
%         make the accelerate handle some global value and never deal with it
%         again. I don't think it makes sense to have more than one @AccHandle@
%         anyway. Stash it in @AccFFI.h@? Alternatively, rename things and don't
%         call @hs_init@. \rob{While accelerateInit can only be called once,
%         accelerateCreate can be called multiple times allowing support for
%         multiple devices, contexts, etc. I added accelerateInit as a convenience
%         function for the most common scenario of only using one device.}
%         \trev{aah, I didn't see the create function there.}
%
%     \item I think the compile/run functions should be combined, which matches
%         the behaviour of run1; it's a bit boring to have to remember to compile
%         a function first (perhaps not for a C programmer?). A @static@ stable
%         pointer in the execution function? \rob{It wouldn't be hard to combine
%         to combine the two. I just wasn't sure whether I should as it didn't
%         seem very C like. However, I think you said OpenCV compiles a function
%         the first time it is executed? If they can get away with it, then I
%         suppose we can as well.}
% \end{itemize}

%\rob{Perhaps we should change this section title? I'm not sure it really matches what we describe.}

\subsubsection{Exporting Accelerate programs}

To export Accelerate functions as C functions, we make use of Template Haskell~\cite{Sheard:2002template}. For example, we might export our Accelerate dot product:
%
\begin{lstlisting}
  dotp :: Acc (Vector Float, Vector Float) -> Acc (Scalar Float)
  dotp = uncurry $ \xs ys -> fold (+) 0 (zipWith (*) xs ys)

  exportAfun 'dotp "dotp_compile"
\end{lstlisting}%
%
The function @exportAfun@ is defined in Template Haskell and takes the name of an Accelerate function, here @dotp@, as an argument. It generates the necessary export declarations by inspecting the properties of the name it has been passed, such as its type.

Compiling a module that exports Accelerate computations in this way (say,
@M.hs@) generates the additional file @M_stub.h@ containing the C prototype for
the foreign exported function. For the dot product example, this header contains:
%
\begin{lstlisting}
  #include "HsFFI.h"
  extern AccProgram dotp_compile(AccContext a1);
\end{lstlisting}%
%
A C program needs to include this header to call the Accelerate dot product.


\subsubsection{Running embedded Accelerate programs}

One of the functions to execute an Accelerate computation in Haskell is:
%
\begin{lstlisting}
  run1In :: (Arrays as, Arrays bs)
         => Context -> (Acc as -> Acc bs) -> as -> bs
\end{lstlisting}%
%
This function comprises two phases: (1) program optimisation and instantiation of skeleton templates of its second argument and (2) execution of the compiled code in a given CUDA context (first argument). The implementation of @run1In@ is structured such that, partially applying it to only its first and second argument, yields a new function of type @as -> bs@, where Phase (1) has been executed already --- in other words, it precompiles the Accelerate code. Repeated application of this function of type @as -> bs@ executes the CUDA code without any of the overheads associated with just-in-time compilation.

The Accelerate export API retains the ability to precompile Accelerate code. The C function provided by @exportAfun@ compiles the Accelerate code, returning a reference to the compiled code. Then, in a second step, @runProgram@ marshals input arrays, executes the compiled program, and marshals output arrays:
%
\begin{lstlisting}
  OutputArray   out;
  InputArray    in[2]   = { ... };
  AccProgram    dotp    = dotp_compile( context );

  runProgram( dotp, in, &out );
\end{lstlisting}%
%
The function @dotp_compile@ was generated by @exportAfun 'dotp "dotp_compile"@.

\subsubsection{Marshalling input and output arrays}

Accelerate uses a non-parametric representation of multi-dimensional arrays: an array of tuples is represented as a tuple of arrays. The type @InputArray@ follows this convention. It is a C struct comprising an array of integers indicating the extent of the array in each dimension together with an array of pointers to each underlying GPU array of primitive data.
%
\begin{lstlisting}
  typedef struct { int* shape; void** adata; } InputArray;
\end{lstlisting}
%

@OutputArray@ includes an extra field, a stable pointer, that maintains a reference to the associated Haskell-side @Array@. This keeps the array from being garbage collected until the @OutputArray@ is explicitly released with @freeOutput@.
%
\begin{lstlisting}
  typedef struct { int* shape; void** adata;
                   HsStablePtr stable_ptr; } OutputArray;
\end{lstlisting}
%


\section{GPU aware garbage collection}
\label{sec:gpu-gc}

One major design consideration of high-level languages is how to manage memory. Forcing the programmer to manually allocate and free memory hardly satisfies the property of being high-level, but any alternative involves some compromise. The most common solution to this problem is automatic garbage collection where the runtime system associated with a language will periodically scan the heap and determine what allocated memory is reachable and free any allocations that are not. There are many different methods for doing this, but what we propose here is agnostic to which method is used.

The Glasgow Haskell Compile (GHC) supports garbage collection already. As Accelerate is embeddded in Haskell, we can introduce garbage collection to it by hooking into GHC's garbage collector. To do this however, we have to address the question of when data should be transferred both to and from the GPU.

\subsection{When to transfer data?}

As we have already highlighted, the limited working memory provided by GPUs makes it difficult to write programs that have inputs of sizes exceeding this memory. We have shown how array sequences can remove this limitation, for certain classes of programs. However, even with sequences, there are still cases where this is a problem. To see why, suppose we have program pipeline with 2 stages. The first stage uses sequences to compute values intended for the second stage. The second stage, however, is not able to process its input as a stream but instead requires all of the results before it can do anything. Such a program might have this structure:
%
\begin{lstlisting}
secondStage . streamOut . firstStage
\end{lstlisting}

For programs like this, the Accelerate runtime system needs to decide when to copy arrays to the \emph{device} memory on the GPU, when to copy them back to \emph{host} memory and when to free the device memory allocated for them. For our example, the first stage could do as follows for each chunk of the sequence:
%
\begin{enumerate}
\item Compute the chunk from @firstStage@.
\item Copy it back to main memory.
\item Free it from GPU memory.
\end{enumerate}
%
This has the advantage of minimizing space usage on device memory, but it is not always the most optimal thing to do. If @secondStage@ is purely CPU based processing, then we have not lost anything by transferring the chunk as soon as we have computed it. If, however, @secondStage@ does its own GPU processing, with some or all of its input, then we might not want to transfer immediately. If it needs that data to once again be in device memory, then it makes little sense to transfer to host memory and back again.

\subsection{The previous solution}

Prior to our work, Accelerate already had a solution to this problem. The transfer of arrays from device memory were, and still are, delayed till an attempt is made to access the values in the array from the CPU. For freeing the array, Accelerate hooks into the Garbage Collector (GC) of the Glasgow Haskell Compiler (GHC). It attaches a finaliser that frees the array in GPU memory when the host-side version of the array is collected.

This solves the problem of premature data transfer. Arrays won't be transferred back to host memory till they're needed there and won't be left in device memory once the host-side array is no longer needed. However, it introduces another problem as device memory usage will grow with host memory usage. Results from @firstStage@ aren't from device memory till they are all computed. Naturally, this leads to memory exhaustion and takes away one of the advantages of streaming (at least for examples like this one.)

\subsection{Array eviction}

As part of this work, we implemented a more advanced form of GPU aware garbage collection. It works in much the same way as the previous solution, but allows for arrays to be copied (\emph{evicted}) back from device memory when memory is exhausted. Specifically, allocation of arrays works as follows:
%
\begin{enumerate}
\item Attempt to allocate space on the device. If that is successful, use it for the device-side representation of the array.
\item If it fails due to an out-of-memory-error, trigger a GHC garbage collection, try to yield to the GC thread, and then try to allocate again.
\item If that fails, find the \emph{least recently used} array in device memory and, if necessary, copy it back to host memory before freeing it. After that, try to allocate again.
\item If that fails, repeat the above step.
\end{enumerate}
%
For (1), we use CUDAs version of @malloc()@ which will fail with an error if is unable to allocate on device memory. With (2) we rely on @performGC@ from @System.Mem@, which causes a GC to occur in a separate thread. We can't yield to this thread exclusively, but, in practice, calling @yield@ from @Control.Concurrent@ typically achieves that.

To do (3) requires us to keep track of when arrays were used. To do this we attach a timestamp to each record of a device-side array. In Section~\ref{sec:foreign-import} we introduced @withDevicePtr@; internally Accelerate uses that function whenever it needs to access an array on the device. We extended that function to update the timestamp associated with the array.

\subsection{Discussion}

This new method of memory management is not only advantageous to programming with sequences, but also more generally. Programs that do not use sequences, but do require inputs larger than available memory also benefit from automatic eviction. That said, an LRU based eviction is not always the most optimal. One example is if a program works by doing multiple passes over the same input, then evicting the least recently used array may not always be the best choice, as it may represent future input. It may be better in that case to evict the result, assuming it is not needed for the next pass of the algorithm. We have found, however, that in most cases the LRU strategy is the best, but there is room for further exploration in this area.